{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TF-IDF_SVM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RuchiChourasia/Deep-learning-with-Python/blob/master/TF-IDF%20with%20SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "zoJPKXYPEabY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Import All Required Packages**"
      ]
    },
    {
      "metadata": {
        "id": "CyIpr5YwZwGi",
        "colab_type": "code",
        "outputId": "8141d802-db00-40c3-bbcc-e1deb24acd51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RuNPM8hvZlrM",
        "colab_type": "code",
        "outputId": "c30661d4-9bfc-4c75-cdb6-adf114598868",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84
        }
      },
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# used for encoding\n",
        "import chardet\n",
        "# natural language toolkit\n",
        "import nltk\n",
        "# wordnet is the database of English language\n",
        "nltk.download('wordnet')\n",
        "# stopwords for removing it from review\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "##from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "# \n",
        "from nltk.util import ngrams\n",
        "import string\n",
        "import math\n",
        "# importing Linear SVM Classifier \n",
        "from sklearn.svm import LinearSVC\n",
        "from nltk.classify import SklearnClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import accuracy_score\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jUAERBpsZlrV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#universal variable decleration\n",
        "dataset=[]\n",
        "trainData=[]\n",
        "testData=[]\n",
        "trainData_label=[]\n",
        "testData_label=[]\n",
        "\n",
        "\n",
        "trainDataAll=[]\n",
        "testDataAll=[]\n",
        "trainData_labelAll=[]\n",
        "testData_labelAll=[]\n",
        "\n",
        "\n",
        "table = str.maketrans({key: None for key in string.punctuation})\n",
        "\n",
        "label=[]\n",
        "#data=[]\n",
        "\n",
        "tfs = {}\n",
        "tfids = {}\n",
        "tfidf = {}\n",
        "\n",
        "tfs_test = {}\n",
        "tfids_test = {}\n",
        "tfidf_test = {}\n",
        "\n",
        "featureDict = {}\n",
        "featureDict_test = {}\n",
        "# A global dictionary of features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Aub6OQ7EZlra",
        "colab_type": "code",
        "outputId": "ccf32094-c0e2-4e49-f542-dd14bbd74a85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "cell_type": "code",
      "source": [
        "#df = pd.read_csv('final amazon review datas.csv', encoding='Latin-1')\n",
        "# r\"/content/drive/My Drive/Colab Notebooks/amazon_dataset_final.csv\"\n",
        "df = pd.read_csv(r\"/content/drive/My Drive/Colab Notebooks/amazon_data.csv\", encoding='Latin-1')\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "column_name =df.columns.tolist()\n",
        "column_name=column_name[:-1]\n",
        "#print(column_name)\n",
        "df = df.replace({'LABEL': {'__label1__':'FAKE' , '__label2__':'REAL'}})\n",
        "label=df['LABEL'].tolist()\n",
        "#label=label.reshape(-1,1)\n",
        "df=df.drop('LABEL',axis=1)\n",
        "display(df.head())\n",
        "        #display(rawData)\n",
        "row,col = df.shape\n",
        "#display(row,col)\n",
        "#print(len(label))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DOC_ID</th>\n",
              "      <th>RATING</th>\n",
              "      <th>VERIFIED_PURCHASE</th>\n",
              "      <th>PRODUCT_CATEGORY</th>\n",
              "      <th>PRODUCT_ID</th>\n",
              "      <th>PRODUCT_TITLE</th>\n",
              "      <th>REVIEW_TITLE</th>\n",
              "      <th>REVIEW_TEXT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>16899</td>\n",
              "      <td>5</td>\n",
              "      <td>Y</td>\n",
              "      <td>Wireless</td>\n",
              "      <td>B00V856N3Y</td>\n",
              "      <td>iPhone 6s Case, TEAM LUXURY [Clarity Series] U...</td>\n",
              "      <td>I absolutely LOVE this case</td>\n",
              "      <td>I absolutely LOVE this case. it arrived very q...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>17672</td>\n",
              "      <td>2</td>\n",
              "      <td>Y</td>\n",
              "      <td>Lawn and Garden</td>\n",
              "      <td>B008NT2KOE</td>\n",
              "      <td>Lacrosse Color Wireless Automic Weather Statio...</td>\n",
              "      <td>***Long term review***</td>\n",
              "      <td>We bought this for my father for Christmas 201...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>16054</td>\n",
              "      <td>4</td>\n",
              "      <td>Y</td>\n",
              "      <td>Tools</td>\n",
              "      <td>B001ID4ZY0</td>\n",
              "      <td>SE CC4580 MilitaryLensatic/Prismatic Sighting ...</td>\n",
              "      <td>Great</td>\n",
              "      <td>Classic style compass just like I grew up with...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8313</td>\n",
              "      <td>5</td>\n",
              "      <td>N</td>\n",
              "      <td>Video DVD</td>\n",
              "      <td>B00RGQ4674</td>\n",
              "      <td>Outlander: Season One - Volume One</td>\n",
              "      <td>I like it!</td>\n",
              "      <td>I read the books and have enjoyed the series.I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>14294</td>\n",
              "      <td>2</td>\n",
              "      <td>Y</td>\n",
              "      <td>Video DVD</td>\n",
              "      <td>B000KBKPIG</td>\n",
              "      <td>Pursuit of Honor - The Rise of George Washington</td>\n",
              "      <td>Nicely done.  Very informative.</td>\n",
              "      <td>We enjoyed this video as a family.  However, w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   DOC_ID  RATING VERIFIED_PURCHASE PRODUCT_CATEGORY  PRODUCT_ID  \\\n",
              "0   16899       5                 Y         Wireless  B00V856N3Y   \n",
              "1   17672       2                 Y  Lawn and Garden  B008NT2KOE   \n",
              "2   16054       4                 Y            Tools  B001ID4ZY0   \n",
              "3    8313       5                 N        Video DVD  B00RGQ4674   \n",
              "4   14294       2                 Y        Video DVD  B000KBKPIG   \n",
              "\n",
              "                                       PRODUCT_TITLE  \\\n",
              "0  iPhone 6s Case, TEAM LUXURY [Clarity Series] U...   \n",
              "1  Lacrosse Color Wireless Automic Weather Statio...   \n",
              "2  SE CC4580 MilitaryLensatic/Prismatic Sighting ...   \n",
              "3                 Outlander: Season One - Volume One   \n",
              "4   Pursuit of Honor - The Rise of George Washington   \n",
              "\n",
              "                      REVIEW_TITLE  \\\n",
              "0      I absolutely LOVE this case   \n",
              "1           ***Long term review***   \n",
              "2                            Great   \n",
              "3                       I like it!   \n",
              "4  Nicely done.  Very informative.   \n",
              "\n",
              "                                         REVIEW_TEXT  \n",
              "0  I absolutely LOVE this case. it arrived very q...  \n",
              "1  We bought this for my father for Christmas 201...  \n",
              "2  Classic style compass just like I grew up with...  \n",
              "3  I read the books and have enjoyed the series.I...  \n",
              "4  We enjoyed this video as a family.  However, w...  "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "WuX-NaUwZlrh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def parse(review):\n",
        "    return (review[7])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uHzedBbhUAyW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def parseAll(review):\n",
        "  #reviewLine[0], reviewLine[2], reviewLine[3],reviewLine[4], reviewLine[7], label\n",
        "  return (review[1],review[2],review[3],review[6],review[7])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VE87otXIZlrn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# TEXT PREPROCESSING AND FEATURE VECTORIZATION\n",
        "# Input: a string of one review\n",
        "#t={key: None for key in string.punctuation} #It will return none for puntuations\n",
        "\n",
        "\n",
        "def preprocess(text):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    filtered_tokens=[]\n",
        "    lemmatized_tokens = []\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = text.translate(table) # translation of text is done here punctuations are replaced with none\n",
        "    for w in text.split(\" \"):\n",
        "        if w not in stop_words:\n",
        "            lemmatized_tokens.append(lemmatizer.lemmatize(w.lower()))\n",
        "            \n",
        "    \n",
        "        #print(lemmatized_tokens)\n",
        "        #filtered_tokens = [' '.join(l) for l in nltk.bigrams(lemmatized_tokens)] + lemmatized_tokens #nltk bigrams will make pair of word occur together mrore frequently\n",
        "        #here ' '.join(l) means it will join the words from the list (here it is l of iterable) with ' ' delimeter in between. \n",
        "    return lemmatized_tokens #lemmatized_tokens  #filtered_tokens\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XxFnw2QUZlsh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def bow(Text):\n",
        "    #print(Text)\n",
        "    bowCount = len(Text)\n",
        "    #display(bowCount)\n",
        "    return bowCount\n",
        "\n",
        "def computeTF(localDict, bowCount):\n",
        "    tfDict = {}\n",
        "    #bowCount = len(bow)\n",
        "    for word, count in localDict.items():\n",
        "        tfDict[word] = count/float(bowCount)\n",
        "    #display(tfDict)\n",
        "    return tfDict\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mrcX27lXZlrt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def computeIDF(docList):\n",
        "   \n",
        "    idfDict = {}\n",
        "    #print(docList)\n",
        "    N = row ## row\n",
        "    #display(N)\n",
        "    \n",
        "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
        "    for doc in docList:\n",
        "      for key, value in doc.items():\n",
        "        \n",
        "        if value > 0:\n",
        "          idfDict[key] += 1\n",
        "              #print(idfDict)\n",
        "   \n",
        "    for word, val in idfDict.items():\n",
        "      idfDict[word] = math.log10(N / float(val))\n",
        "   \n",
        "    return idfDict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XOy3FsKKdg01",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def computeTFIDF(tfs, idfs): \n",
        "  \n",
        "  \n",
        "  for key,val in tfs.items():\n",
        "    t={}\n",
        "    for k,v in val.items():\n",
        "       t[k]=v*idfs[k]\n",
        "    tfidf[key] = t\n",
        "      #tfidf[val] = val\n",
        "  #display(tfidf)\n",
        "  \n",
        "  return tfidf\n",
        "  \n",
        "def computeTFIDF_test(tfs_test, idfs_t): \n",
        "  \n",
        "  \n",
        "  for key,val in tfs_test.items():\n",
        "    t1={}\n",
        "    for k,v in val.items():\n",
        "       t1[k]=v*idfs_t[k]\n",
        "    tfidf_test[key] = t1\n",
        "      #tfidf[val] = val\n",
        "  #display(tfidf)\n",
        "  \n",
        "  return tfidf_test\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OwaRcuQbZlsX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def toFeatureVector(tokens):\n",
        "    localDict = {}\n",
        "    for token in tokens:\n",
        "        if token not in featureDict:\n",
        "            featureDict[token] = 1\n",
        "        else:\n",
        "            featureDict[token] += 1\n",
        "   \n",
        "        if token not in localDict:\n",
        "            localDict[token] = 1\n",
        "        else:\n",
        "            localDict[token] += 1\n",
        "    \n",
        "    \n",
        "    return localDict\n",
        "def toFeatureVector_test(tokens1):\n",
        "    localDict1 = {}\n",
        "    for token1 in tokens1:\n",
        "        if token1 not in featureDict_test:\n",
        "            featureDict_test[token1] = 1\n",
        "        else:\n",
        "            featureDict_test[token1] += 1\n",
        "   \n",
        "        if token1 not in localDict1:\n",
        "            localDict1[token1] = 1\n",
        "        else:\n",
        "            localDict1[token1] += 1\n",
        "    \n",
        "    \n",
        "    return localDict1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TMjJFLBoZX3c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oom-lKf3ZlsI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# parse dataset and extract essential feature from it\n",
        "\n",
        "## row ##\n",
        "def tfidf_compute(dataset1):\n",
        "  length=len(dataset1)\n",
        "  for i in range(length):\n",
        "      Text = parse(dataset1[i])\n",
        "      temp={}\n",
        "      #data.append((Label,Text))\n",
        "      ##data.append((Label, computeTF(toFeatureVector(preprocess(Text)),bow(preprocess(Text)))))\n",
        "      temp={i:computeTF(toFeatureVector(preprocess(Text)),bow(preprocess(Text)))}\n",
        "      tfs.update(temp)\n",
        "\n",
        "  idfs = computeIDF([featureDict])\n",
        "  tfidfs = computeTFIDF(tfs, idfs)\n",
        "  return tfidfs\n",
        "\n",
        "def tfidf_compute_test(dataset2):\n",
        "  length1=len(dataset2)\n",
        "  for j in range(length1):\n",
        "      Text1 = parse(dataset2[j])\n",
        "      temp1={}\n",
        "      #data.append((Label,Text))\n",
        "      ##data.append((Label, computeTF(toFeatureVector(preprocess(Text)),bow(preprocess(Text)))))\n",
        "      temp1={j:computeTF(toFeatureVector_test(preprocess(Text1)),bow(preprocess(Text1)))}\n",
        "      tfs_test.update(temp1)\n",
        "\n",
        "  idfs_t = computeIDF([featureDict_test])\n",
        "  tfidfs_test = computeTFIDF_test(tfs_test, idfs_t)\n",
        "  return tfidfs_test\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XoXopEefbUGE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tfidf_computeAll(dataset1):\n",
        "  length=len(dataset1)\n",
        "  for i in range(length):\n",
        "      R,VP,PC,Title,Text = parseAll(dataset1[i])\n",
        "      temp={}\n",
        "      #data.append((Label,Text))\n",
        "      ##data.append((Label, computeTF(toFeatureVector(preprocess(Text)),bow(preprocess(Text)))))\n",
        "      #temp={i:computeTF(toFeatureVector(preprocess(R+\" \"+VP+\" \"+PC+\" \"+Title+\" \"+Text)),bow(preprocess(R+\" \"+VP+\" \"+PC+\" \"+Title+\" \"+Text)))}\n",
        "      temp={i:computeTF(toFeatureVector(preprocess(f'{R} {VP} {PC} {Title} {Text}')),bow(preprocess(f'{R} {VP} {PC} {Title} {Text}')))}\n",
        "      \n",
        "      tfs.update(temp)\n",
        "\n",
        "  idfs = computeIDF([featureDict])\n",
        "  tfidfs = computeTFIDF(tfs, idfs)\n",
        "  return tfidfs\n",
        "\n",
        "def tfidf_compute_testAll(dataset2):\n",
        "  length1=len(dataset2)\n",
        "  for j in range(length1):\n",
        "      R,VP,PC,Title,Text = parseAll(dataset2[j])\n",
        "      temp1={}\n",
        "      #data.append((Label,Text))\n",
        "      ##data.append((Label, computeTF(toFeatureVector(preprocess(Text)),bow(preprocess(Text)))))\n",
        "      temp1={j:computeTF(toFeatureVector_test(preprocess(f'{R} {VP} {PC} {Title} {Text}')),bow(preprocess(f'{R} {VP} {PC} {Title} {Text}')))}\n",
        "      tfs_test.update(temp1)\n",
        "\n",
        "  idfs_t = computeIDF([featureDict_test])\n",
        "  tfidfs_test = computeTFIDF_test(tfs_test, idfs_t)\n",
        "  return tfidfs_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cGCVA0e6ZlsA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset=list(df.values)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r6a9JJyNP1ms",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def split(percent):\n",
        "    dataSamples = len(dataset)\n",
        "    halfOfData = int(len(dataset)/2)\n",
        "    trainingSamples = int((percent*dataSamples)/2)\n",
        "    \n",
        "  \n",
        "    x1=dataset[:trainingSamples] + dataset[halfOfData:halfOfData+trainingSamples]\n",
        "    train_tfidf = tfidf_compute(x1)\n",
        "  \n",
        "    for m,n in train_tfidf.items():\n",
        "      trainData.append((n))\n",
        "    \n",
        "   \n",
        "    for x in label[:trainingSamples]+label[halfOfData:halfOfData+trainingSamples]:\n",
        "      trainData_label.append(x)\n",
        "    \n",
        "    # Test data here\n",
        "    \n",
        "    x2=dataset[trainingSamples:halfOfData] + dataset[halfOfData+trainingSamples:]\n",
        "    test_tfidf=tfidf_compute_test(x2)\n",
        "    \n",
        "   \n",
        "    for m1,n1 in test_tfidf.items():\n",
        "      testData.append((n1))\n",
        "    \n",
        "   \n",
        "    for y in label[trainingSamples:halfOfData] + label[halfOfData+trainingSamples:]:\n",
        "      testData_label.append(y)\n",
        "    \n",
        "    \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dymfvRkWZlr4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "826596eb-b219-4211-9925-a3fb59b8a596"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'': 0.27013870592086997,\n",
              " 'able': 0.27013870592086997,\n",
              " 'clutch': 0.5402774118417399,\n",
              " 'fine': 0.27013870592086997,\n",
              " 'fit': 0.27013870592086997,\n",
              " 'i': 0.27013870592086997,\n",
              " 'needed': 0.27013870592086997,\n",
              " 'poulan': 0.27013870592086997,\n",
              " 'remove': 0.27013870592086997,\n",
              " 'repair': 0.27013870592086997,\n",
              " 'this': 0.27013870592086997,\n",
              " 'tool': 0.5402774118417399,\n",
              " 'wacker': 0.27013870592086997,\n",
              " 'weed': 0.27013870592086997}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "1QIzDrBtJawL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Train Data and Test Data**"
      ]
    },
    {
      "metadata": {
        "id": "N3KFSa6qZlst",
        "colab_type": "code",
        "outputId": "f90eed9c-c1cb-4afd-a85f-6188635b9eb0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "#print(len(trainData),len(trainData_label))\n",
        "TRAIN_MAIN_DATA=np.c_[trainData,trainData_label]\n",
        "TEST_MAIN_DATA=np.c_[testData,testData_label]\n",
        "TRAIN_MAIN_DATA=TRAIN_MAIN_DATA.tolist()\n",
        "TEST_MAIN_DATA=TEST_MAIN_DATA.tolist()\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "19950 19950\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nvmlQY2DJktV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**SPLIT Function for All types of Feature Input**"
      ]
    },
    {
      "metadata": {
        "id": "jw5tqDkr0fPA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def splitAll(percent):\n",
        "    dataSamples = len(dataset)\n",
        "    halfOfData = int(len(dataset)/2)\n",
        "    trainingSamples = int((percent*dataSamples)/2)\n",
        "    \n",
        "    # Train Data here  \n",
        "  \n",
        "    x1=dataset[:trainingSamples] + dataset[halfOfData:halfOfData+trainingSamples]\n",
        "    train_tfidf = tfidf_computeAll(x1)\n",
        "  \n",
        "    for m,n in train_tfidf.items():\n",
        "      trainDataAll.append((n))\n",
        "    \n",
        "   \n",
        "    for x in label[:trainingSamples]+label[halfOfData:halfOfData+trainingSamples]:\n",
        "      trainData_labelAll.append(x)\n",
        "    \n",
        "    # Test Data here\n",
        "    \n",
        "    x2=dataset[trainingSamples:halfOfData] + dataset[halfOfData+trainingSamples:]\n",
        "    test_tfidf=tfidf_compute_testAll(x2)\n",
        "    \n",
        "   \n",
        "    for m1,n1 in test_tfidf.items():\n",
        "      testDataAll.append((n1))\n",
        "    \n",
        "   \n",
        "    for y in label[trainingSamples:halfOfData] + label[halfOfData+trainingSamples:]:\n",
        "      testData_labelAll.append(y)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "26o01LKv0faY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "splitAll(0.95)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zkY2roam1mrg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "TRAIN_MAIN_DATA_All=np.c_[trainDataAll,trainData_label]\n",
        "TEST_MAIN_DATA_All=np.c_[testDataAll,testData_label]\n",
        "TRAIN_MAIN_DATA_All=TRAIN_MAIN_DATA_All.tolist()\n",
        "TEST_MAIN_DATA_All=TEST_MAIN_DATA_All.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8QIQBPPD6G-g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d35ff4a8-ec18-44de-aebe-1871ddafbf94"
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "19950"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "metadata": {
        "id": "Ncai1h1_Zlsn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def trainClassifier(trainclassifier):\n",
        "    print(\"Training Classifier\")\n",
        "    pipeline =  Pipeline([('svc', LinearSVC())])\n",
        "    return SklearnClassifier(pipeline).train(trainclassifier)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wdpdtRg0Uyvy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def crossValidate(dataset_cv, folds):\n",
        "    cv_results = []\n",
        "    foldSize = int(len(dataset_cv)/folds)\n",
        "    for i in range(0,len(dataset_cv),foldSize):\n",
        "        classifier = trainClassifier(dataset_cv[:i]+dataset_cv[foldSize+i:])\n",
        "        y_pred = predictLabels(dataset_cv[i:i+foldSize],classifier)\n",
        "        a = accuracy_score(list(map(lambda d : d[1], dataset_cv[i:i+foldSize])), y_pred)\n",
        "        (p,r,f,_) = precision_recall_fscore_support(list(map(lambda d : d[1], dataset_cv[i:i+foldSize])), y_pred, average ='macro')\n",
        "        #print(a,p,r,f)\n",
        "        cv_results.append((a,p,r,f))\n",
        "    cv_results = (np.mean(np.array(cv_results),axis=0))\n",
        "    return cv_results"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4du4lDisZlry",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predictLabels(reviewSamples, classifier1):\n",
        "    return classifier1.classify_many(map(lambda t: t[0], reviewSamples))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NNlxUNkPVBNJ",
        "colab_type": "code",
        "outputId": "bd2e57ba-17fe-4f78-9b69-f5350aa79f7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Mean of cross-validations (Accuracy, Precision, Recall, Fscore): \", crossValidate(TRAIN_MAIN_DATA, 10))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Mean of cross-validations (Accuracy, Precision, Recall, Fscore):  [0.67909774 0.67910542 0.67907776 0.67899796]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wCXQLAasshOc",
        "colab_type": "code",
        "outputId": "b69d7f06-c405-4b4e-e937-f3ccdca836ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "cell_type": "code",
      "source": [
        "## Test Accuracy\n",
        "classifier = trainClassifier(TRAIN_MAIN_DATA)\n",
        "predictions = predictLabels(TEST_MAIN_DATA, classifier)\n",
        "true_labels = list(map(lambda d: d[1], TEST_MAIN_DATA))\n",
        "a = accuracy_score(true_labels, predictions)\n",
        "p, r, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='macro')\n",
        "print(\"accuracy: \", a)\n",
        "print(\"Precision: \", p)\n",
        "print(\"Recall: \", r)\n",
        "print(\"f1-score: \", f1)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Classifier\n",
            "accuracy:  0.7047619047619048\n",
            "Precision:  0.7050904287746393\n",
            "Recall:  0.7054482798732079\n",
            "f1-score:  0.7046933346881452\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XVZV_jnH2-ta",
        "colab_type": "code",
        "outputId": "1dc06978-e764-43f9-e494-15cfd33edc14",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"Mean of cross-validations (Accuracy, Precision, Recall, Fscore): \", crossValidate(TRAIN_MAIN_DATA_All, 10))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Training Classifier\n",
            "Mean of cross-validations (Accuracy, Precision, Recall, Fscore):  [0.82235589 0.82385949 0.82246602 0.82212574]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_1v65dJc6Nsb",
        "colab_type": "code",
        "outputId": "d1889844-854d-4ecf-845f-2ddc4dbdbcab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101
        }
      },
      "cell_type": "code",
      "source": [
        "## Test Accuracy\n",
        "classifier_All = trainClassifier(TRAIN_MAIN_DATA_All)\n",
        "predictions_All = predictLabels(TEST_MAIN_DATA_All, classifier_All)\n",
        "true_labels_All = list(map(lambda d: d[1], TEST_MAIN_DATA_All))\n",
        "a_All = accuracy_score(true_labels_All, predictions_All)\n",
        "p_All, r_All, f1_All, _ = precision_recall_fscore_support(true_labels_All, predictions_All, average='macro')\n",
        "print(\"accuracy: \", a_All)\n",
        "print(\"Precision: \", p_All)\n",
        "print(\"Recall: \", r_All)\n",
        "print(\"f1-score: \", f1_All)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Classifier\n",
            "accuracy:  0.8219047619047619\n",
            "Precision:  0.8229029645941499\n",
            "Recall:  0.8203499869136592\n",
            "f1-score:  0.820991452991453\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xXgrR5sCZxZm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VLJs3E2PZlsR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}